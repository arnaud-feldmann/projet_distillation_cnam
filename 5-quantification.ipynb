{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kc8YkE46d5Sk",
    "outputId": "cfd3c208-2b49-47aa-a433-f2ea43363125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_model_optimization in /opt/mamba/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: tf_keras in /opt/mamba/lib/python3.12/site-packages (2.16.0)\n",
      "Requirement already satisfied: image_classifiers in /opt/mamba/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /opt/mamba/lib/python3.12/site-packages (from tensorflow_model_optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/mamba/lib/python3.12/site-packages (from tensorflow_model_optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in /opt/mamba/lib/python3.12/site-packages (from tensorflow_model_optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /opt/mamba/lib/python3.12/site-packages (from tensorflow_model_optimization) (1.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/mamba/lib/python3.12/site-packages (from tf_keras) (2.16.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/mamba/lib/python3.12/site-packages (from image_classifiers) (1.0.8)\n",
      "Requirement already satisfied: h5py in /opt/mamba/lib/python3.12/site-packages (from keras-applications<=1.0.8,>=1.0.7->image_classifiers) (3.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (69.5.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/mamba/lib/python3.12/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.3.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/mamba/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/mamba/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/mamba/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/mamba/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/mamba/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/mamba/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/mamba/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/mamba/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/mamba/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/mamba/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 11:19:55.699564: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-03 11:19:55.700563: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 11:19:55.707232: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 11:19:55.775016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 11:19:56.788591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_model_optimization tf_keras image_classifiers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "from tf_keras.applications import ResNet50V2\n",
    "from tf_keras.datasets import cifar100\n",
    "from tf_keras import Sequential, Input\n",
    "from tf_keras.layers import Dense, Dropout, RandomFlip, RandomTranslation, RandomRotation,RandomBrightness, RandomContrast, RandomZoom, GlobalAveragePooling2D\n",
    "from tf_keras.applications.resnet_v2 import preprocess_input\n",
    "from tf_keras.losses import CategoricalCrossentropy\n",
    "from tf_keras.activations import linear\n",
    "from tf_keras.models import Model\n",
    "from tf_keras.backend import clear_session\n",
    "from tf_keras.optimizers import SGD\n",
    "from tf_keras.utils import Progbar\n",
    "from tensorflow.nn import softmax_cross_entropy_with_logits\n",
    "from classification_models.models_factory import ModelsFactory\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras.pruning_wrapper import PruneLowMagnitude\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelsFactory(ModelsFactory):\n",
    "    @staticmethod\n",
    "    def get_kwargs():\n",
    "        return {\n",
    "            'backend': keras.backend,\n",
    "            'layers': keras.layers,\n",
    "            'models': keras.models,\n",
    "            'utils': keras.utils,\n",
    "        }\n",
    "# Pour faire marcher image-classifiers avec tf_keras, la version compatibilité de tf.keras, nécéssaire pour tflite\n",
    "Classifiers = KerasModelsFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 100\n",
    "num_classes = 100\n",
    "n_images = 50000 # Pour l'entrainement, et 10000 pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uPDP8KyWfCat"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...nant.keras: 135.73 MiB / 135.73 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 91.17 MiB/s 1s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc cp s3/afeldmann/projet_cnam/modele_enseignant.keras /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\n",
    "model_enseignant = Sequential([\n",
    "    Input((224,224,3)),\n",
    "    ResNet50V2(include_top=False, weights='imagenet', pooling=\"avg\"),\n",
    "    Dropout(0.25),\n",
    "    Dense(256, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.L1(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\", kernel_regularizer = keras.regularizers.L2(0.001))\n",
    "])\n",
    "# Keras 3.1.1 est buggé et le chargement direct ne marche pas ici, même si les poids sont bien enregistrés\n",
    "model_enseignant.load_weights(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\")\n",
    "\n",
    "model_enseignant.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    resnet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    resnet = resnet18((224, 224, 3), weights='imagenet', include_top=False)\n",
    "    resnet_output = GlobalAveragePooling2D()(resnet.output)\n",
    "    resnet = Model(inputs=resnet.input, outputs=resnet_output)\n",
    "    return resnet\n",
    "\n",
    "def new_modele_resnet():\n",
    "    model = Sequential([\n",
    "        Input((224,224,3)),\n",
    "        ResNet18(),\n",
    "        Dropout(0.25),\n",
    "        Dense(256, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.L1(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\", kernel_regularizer = keras.regularizers.L2(0.001))\n",
    "    ])\n",
    "    model.compile(metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..._a50.keras: 43.54 MiB / 43.54 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 62.92 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'RMSprop', because it has 1 variables whereas the saved optimizer has 2 variables. \n"
     ]
    }
   ],
   "source": [
    "!mc cp s3/afeldmann/projet_cnam/model_etudiant_t3_a50.keras /home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\n",
    "model_etudiant = new_modele_resnet()\n",
    "model_etudiant.load_weights(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\")\n",
    "model_etudiant.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bd51EjqzvhP7"
   },
   "outputs": [],
   "source": [
    "def preprocessing(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    label = tf.squeeze(tf.one_hot(label, depth = num_classes), axis = 0)\n",
    "    return  image, label\n",
    "\n",
    "augmentation_donnees_keras = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.2,0.2),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "    RandomContrast(0.2),\n",
    "    RandomBrightness(0.2,value_range=(0,1))\n",
    "])\n",
    "\n",
    "def augmentation_donnees(image, label):\n",
    "    return augmentation_donnees_keras(image/255.0, training = True)*255.0, label\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    return preprocess_input(image), label\n",
    "\n",
    "def load_cifar_train():\n",
    "    train_dataset, _ = cifar100.load_data()\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset).map(preprocessing).cache().repeat().shuffle(n_images).batch(batch_size).map(augmentation_donnees, num_parallel_calls = tf.data.AUTOTUNE).map(preprocess_resnet, num_parallel_calls = tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_dataset\n",
    "\n",
    "_, test_dataset = cifar100.load_data()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_dataset).map(preprocessing).batch(batch_size).map(preprocess_resnet, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modele_logits(modele):\n",
    "    config = modele.layers[-1].get_config()\n",
    "    config['activation'] = linear\n",
    "    config['name'] = 'logits'\n",
    "    res = Model(inputs=modele.inputs, outputs=[Dense(**config)(modele.layers[-2].output)])\n",
    "    res.layers[-1].set_weights([x.numpy() for x in modele.layers[-1].weights])\n",
    "    res.compile(metrics=['accuracy'])\n",
    "    return res\n",
    "\n",
    "def get_regularisation_pruning(model):\n",
    "    return [(getattr(layer.layer,reg),layer.layer.kernel) for layer in model.layers if hasattr(layer, \"layer\") for reg in [\"kernel_regularizer\", \"bias_regularizer\"] if hasattr(layer.layer, reg) and getattr(layer.layer,reg) is not None]\n",
    "\n",
    "@tf.function\n",
    "def perte_regularisation(regularisation):\n",
    "    return tf.add_n([tf.reduce_sum(reg(kernel)) for reg, kernel in regularisation])\n",
    "\n",
    "@tf.function\n",
    "def compte_bons(x,y):\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(tf.argmax(x, axis = 1), tf.argmax(y, axis = 1)), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def softmax(logits, temp):\n",
    "    expo = tf.exp(logits / temp)\n",
    "    return expo / tf.reduce_sum(expo, axis = 1, keepdims=True)\n",
    "\n",
    "kce = keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "# Pour une raison non-identifiée, tf.nn.crossentropy_with_logits se comporte mal pendant le pruning et fait baisser l'accuracy.\n",
    "@tf.function\n",
    "def ce(x, y_logits, temp):\n",
    "    return kce(x, y_logits/temp) * temp**2\n",
    "\n",
    "def init_csv_log(fichier):\n",
    "    with open(fichier,'w') as file:\n",
    "        file.write(\"epoch, accuracy_etu, accuracy_dis\\n\")\n",
    "def append_csv_log(fichier, epoch, accuracy_etu, accuracy_dis):\n",
    "    with open(fichier,'a') as file:\n",
    "        file.write(f\"{epoch:d},{accuracy_etu:.2f},{accuracy_dis:.2f}\\n\")\n",
    "\n",
    "def forward_backward_pass_impl(train_dataset_iter, model_etudiant_pruning, alpha, temp, optim, regularisation_etudiant):\n",
    "    X_batch, y_batch, enseignant_estim_softmax = next(train_dataset_iter)\n",
    "    with tf.GradientTape() as tape:\n",
    "        etudiant_estim_logit = model_etudiant_pruning(X_batch, training = True)\n",
    "        perte = alpha * kce(y_batch,etudiant_estim_logit) + (1-alpha) * ce(enseignant_estim_softmax,etudiant_estim_logit, temp) + perte_regularisation(regularisation_etudiant)\n",
    "    grads = tape.gradient(perte, model_etudiant_pruning.trainable_variables)\n",
    "    optim.apply_gradients(zip(grads, model_etudiant_pruning.trainable_variables))\n",
    "    return compte_bons(etudiant_estim_logit,y_batch), compte_bons(etudiant_estim_logit, enseignant_estim_softmax)\n",
    "\n",
    "def distillateur_kl_pruning(model_etudiant, enseignant, train_dataset, temp, nom_modele, n_epoch, alpha):\n",
    "    etudiant_logit_model = get_modele_logits(model_etudiant)\n",
    "    enseignant_logit_model = get_modele_logits(enseignant)\n",
    "    end_step = np.ceil(n_images / batch_size).astype(np.int32) * n_epoch\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.20,\n",
    "                                                                   final_sparsity=0.60,\n",
    "                                                                   begin_step=0,\n",
    "                                                                   end_step=end_step)\n",
    "    }\n",
    "    model_etudiant_pruning = tfmot.sparsity.keras.prune_low_magnitude(etudiant_logit_model, **pruning_params)\n",
    "    model_etudiant_pruning.compile(optimizer=keras.optimizers.SGD(learning_rate=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    optim = model_etudiant_pruning.optimizer\n",
    "    init_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\")\n",
    "    print(\"C'est parti pour la distillation !\\n\")\n",
    "    train_dataset_iter = iter(\n",
    "        train_dataset\n",
    "        .map(lambda images, label: (images, label, softmax(enseignant_logit_model(images, training = False), temp)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    regularisation_etudiant = get_regularisation_pruning(model_etudiant_pruning)\n",
    "    forward_backward_pass = tf.function(forward_backward_pass_impl)\n",
    "    step_callback = tfmot.sparsity.keras.UpdatePruningStep()\n",
    "    step_callback.set_model(model_etudiant_pruning)\n",
    "    step_callback.on_train_begin()\n",
    "    # La tf.function ne peut être que locale car son graphe dépend d'étudiant_logit_model et sinon Tensorflow renvoie une erreur à deux applications successives\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Époque {epoch + 1} / {n_epoch}\")\n",
    "        step_callback.on_epoch_begin(epoch)\n",
    "        n_batch = n_images//batch_size\n",
    "        barre_progression = Progbar(n_batch, stateful_metrics = [\"acc (etu, train)\", \"acc (dis, train)\"])\n",
    "        bons_epoque_etu, bons_epoque_dis = 0, 0\n",
    "        for i in range(n_batch):\n",
    "            step_callback.on_train_batch_begin(i)\n",
    "            bons_etu, bons_dis = forward_backward_pass(train_dataset_iter, model_etudiant_pruning, alpha, temp, optim, regularisation_etudiant)\n",
    "            bons_epoque_etu += bons_etu.numpy()\n",
    "            bons_epoque_dis += bons_dis.numpy()\n",
    "            n_observ = (i+1) * batch_size\n",
    "            accuracy_etu, accuracy_dis = bons_epoque_etu / n_observ, bons_epoque_dis / n_observ\n",
    "            barre_progression.update(i + 1, values = [(\"acc (etu, train)\", accuracy_etu), (\"acc (dis, train)\", accuracy_dis)])\n",
    "            step_callback.on_train_batch_end(i)\n",
    "        step_callback.on_epoch_end(epoch)\n",
    "        append_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\", epoch, accuracy_etu, accuracy_dis)\n",
    "    step_callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est parti pour la distillation !\n",
      "\n",
      "Époque 1 / 10\n",
      "500/500 [==============================] - 1238s 2s/step - acc (etu, train): 0.6912 - acc (dis, train): 0.6833\n",
      "Époque 2 / 10\n",
      "500/500 [==============================] - 1149s 2s/step - acc (etu, train): 0.7213 - acc (dis, train): 0.7127\n",
      "Époque 3 / 10\n",
      "500/500 [==============================] - 1059s 2s/step - acc (etu, train): 0.7349 - acc (dis, train): 0.7295\n",
      "Époque 4 / 10\n",
      "500/500 [==============================] - 909s 2s/step - acc (etu, train): 0.7315 - acc (dis, train): 0.7248\n",
      "Époque 5 / 10\n",
      "500/500 [==============================] - 908s 2s/step - acc (etu, train): 0.7347 - acc (dis, train): 0.7299\n",
      "Époque 6 / 10\n",
      "500/500 [==============================] - 904s 2s/step - acc (etu, train): 0.7270 - acc (dis, train): 0.7229\n",
      "Époque 7 / 10\n",
      "500/500 [==============================] - 905s 2s/step - acc (etu, train): 0.7266 - acc (dis, train): 0.7216\n",
      "Époque 8 / 10\n",
      "500/500 [==============================] - 921s 2s/step - acc (etu, train): 0.7217 - acc (dis, train): 0.7186\n",
      "Époque 9 / 10\n",
      "500/500 [==============================] - 946s 2s/step - acc (etu, train): 0.7237 - acc (dis, train): 0.7195\n",
      "Époque 10 / 10\n",
      "500/500 [==============================] - 902s 2s/step - acc (etu, train): 0.7257 - acc (dis, train): 0.7222\n",
      "`/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant_pruning.keras` -> `s3/afeldmann/projet_cnam/model_etudiant_pruning.keras`\n",
      "Total: 43.53 MiB, Transferred: 43.53 MiB, Speed: 103.95 MiB/s\n",
      "`/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant_pruning_logs.csv` -> `s3/afeldmann/projet_cnam/model_etudiant_pruning_logs.csv`\n",
      "Total: 154 B, Transferred: 154 B, Speed: 1.61 KiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_cifar_train()\n",
    "nom_modele =  f\"model_etudiant_pruning\"\n",
    "distillateur_kl_pruning(model_etudiant, model_enseignant, train_dataset, 3, nom_modele, n_epoch, 0.5)\n",
    "wd = os.getcwd()\n",
    "model_etudiant.save(f\"{wd}/sauvegardes/{nom_modele}.keras\")\n",
    "os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}.keras s3/afeldmann/projet_cnam/{nom_modele}.keras\")\n",
    "os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}_logs.csv s3/afeldmann/projet_cnam/{nom_modele}_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 34s 272ms/step - loss: 4.0610 - accuracy: 0.7190\n",
      "100/100 [==============================] - 75s 664ms/step - loss: 4.0610 - accuracy: 0.7232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.060976505279541, 0.7232000231742859]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_etudiant.compile(metrics=\"accuracy\")\n",
    "model_etudiant.evaluate(test_dataset)\n",
    "model_etudiant.evaluate(test_dataset.map(lambda images, labels: (images,model_enseignant(images, training = False)), num_parallel_calls = tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nkO80TA8Di2",
    "outputId": "30f808df-b05d-4b70-87db-66fbc0a80dd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9291/3868448782.py:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  keras.models.save_model(model_etudiant, pruned_keras_file, include_optimizer=False)\n"
     ]
    }
   ],
   "source": [
    "pruned_keras_file = \"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.h5\"\n",
    "keras.models.save_model(model_etudiant, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS6JihkL9dcL",
    "outputId": "664d79e8-36bf-42b5-d68b-88e6c1441de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsfqj3ns0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsfqj3ns0/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1714745199.678982    9291 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1714745199.679022    9291 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-03 14:06:39.679533: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsfqj3ns0\n",
      "2024-05-03 14:06:39.689956: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-03 14:06:39.689987: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpsfqj3ns0\n",
      "2024-05-03 14:06:39.754082: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-05-03 14:06:39.763573: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-03 14:06:39.941364: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmpsfqj3ns0\n",
      "2024-05-03 14:06:40.000148: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 320618 microseconds.\n",
      "2024-05-03 14:06:40.084404: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_etudiant)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pruned_tflite_model = converter.convert()\n",
    "pruned_tflite_file = \"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.tflite\"\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-lFfpMf_ncg",
    "outputId": "c8095dae-df05-4214-a4fb-b9b05bed72d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "998it [01:26, 26.13it/s]2024-05-03 14:08:12.809011: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "1000it [01:26, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727 prédictions correctes sur 1000 face aux étiquettes dures et 722 face aux étiquettes douces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_batches = test_dataset.map(lambda images, labels: (images,labels,model_enseignant(images, training = False)), num_parallel_calls = tf.data.AUTOTUNE).unbatch().batch(1)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=pruned_tflite_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_etupreds, test_labels, test_enspreds = [], [], []\n",
    "for img, label, enspred in tqdm(test_batches.take(1000)):\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    test_etupreds.append(interpreter.get_tensor(output_index))\n",
    "    test_labels.append(label.numpy()[0])\n",
    "    test_enspreds.append(enspred.numpy()[0])\n",
    "\n",
    "score_dur, score_dis = 0, 0\n",
    "for item in range(0,len(test_etupreds)):\n",
    "    etupred = np.argmax(test_etupreds[item])\n",
    "    label = np.argmax(test_labels[item])\n",
    "    enspred = np.argmax(test_enspreds[item])\n",
    "    if etupred == label:\n",
    "        score_dur += 1\n",
    "    if etupred == enspred:\n",
    "        score_dis += 1\n",
    "\n",
    "print(f\"{score_dur} prédictions correctes sur 1000 face aux étiquettes dures et {score_dis} face aux étiquettes douces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du modèle enseignant zippé : 131559019.00 bytes\n",
      "Taille du modèle étudiant zippé : 42144643.00 bytes\n",
      "Taille du modèle étudiant zippé après élagage : 21847608.00 bytes\n",
      "Taille du modèle étudiant zippé après élagage et quantification : 6231886.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille du modèle enseignant zippé : %.2f bytes\" % (get_gzipped_model_size(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\")))\n",
    "print(\"Taille du modèle étudiant zippé : %.2f bytes\" % (get_gzipped_model_size(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\")))\n",
    "print(\"Taille du modèle étudiant zippé après élagage : %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Taille du modèle étudiant zippé après élagage et quantification : %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...pruning.h5: 43.42 MiB / 43.42 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 118.09 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc cp /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.tflite s3/afeldmann/projet_cnam/modele_etudiant_pruning.tflite\n",
    "!mc cp /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.h5 s3/afeldmann/projet_cnam/modele_etudiant_pruning.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKJ8xfxqA35dOVM+IFKB3Q",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1zfo5zNxKu097eBmdA-BrPQHD7kJFJxRZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
