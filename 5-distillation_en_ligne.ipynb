{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e32984-de6e-4871-a8f1-d59fa8da7400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: image_classifiers in /opt/mamba/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/mamba/lib/python3.11/site-packages (from image_classifiers) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/mamba/lib/python3.11/site-packages (from keras-applications<=1.0.8,>=1.0.7->image_classifiers) (1.26.4)\n",
      "Requirement already satisfied: h5py in /opt/mamba/lib/python3.11/site-packages (from keras-applications<=1.0.8,>=1.0.7->image_classifiers) (3.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 12:49:48.457866: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-22 12:49:48.537230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 12:49:49.912016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "!pip install image_classifiers\n",
    "#!pip install keras==3.1.1 tensorflow==2.16.1\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.applications import ResNet50V2\n",
    "from keras.datasets import cifar100\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout, RandomFlip, RandomTranslation, RandomRotation,RandomBrightness, RandomContrast, RandomZoom, GlobalAveragePooling2D\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from keras.models import Model\n",
    "from classification_models.keras import Classifiers\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.activations import linear\n",
    "from keras.utils import Progbar\n",
    "from keras.backend import clear_session\n",
    "from tensorflow.nn import softmax_cross_entropy_with_logits\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7017d2c0-9d52-4a38-bde8-b7ae9bb8c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 70\n",
    "batch_size = 64\n",
    "taux_validation = 0.1\n",
    "num_classes = 100\n",
    "n_images = 50000 # Pour l'entrainement, et 10000 pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3214c68f-d468-486d-ba0b-1acadab21793",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = int(n_images * taux_validation)\n",
    "train_size = n_images - validation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b4dd8c-991e-408b-a4ee-fa35dd060c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 12:49:51.148936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13290 MB memory:  -> device: 0, name: NVIDIA A2, pci bus id: 0000:b1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_enseignant = Sequential([\n",
    "    Input((224,224,3)),\n",
    "    ResNet50V2(include_top=False, weights='imagenet', pooling=\"avg\"),\n",
    "    Dropout(0.25),\n",
    "    Dense(256, activation=\"sigmoid\", kernel_regularizer = tf.keras.regularizers.L1(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\", kernel_regularizer = tf.keras.regularizers.L2(0.001))\n",
    "])\n",
    "for layer in model_enseignant.layers:\n",
    "    if layer.name == \"resnet50v2\":\n",
    "        for layer in layer.layers[:]:\n",
    "          if (re.match(\"^.*(_3_conv|_bn)$\", layer.name)):\n",
    "            layer.trainable = True\n",
    "          else:\n",
    "            layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614fdd71-6e71-483a-a206-7a484f057063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    label = tf.squeeze(tf.one_hot(label, depth = num_classes), axis = 0)\n",
    "    return  image, label\n",
    "\n",
    "augmentation_donnees_keras = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.2,0.2),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "    RandomContrast(0.2),\n",
    "    RandomBrightness(0.2,value_range=(0,1))\n",
    "])\n",
    "\n",
    "def augmentation_donnees(image, label):\n",
    "    return augmentation_donnees_keras(image/255.0, training = True)*255.0, label\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    return preprocess_input(image), label\n",
    "\n",
    "def train_val_split(train_dataset, validation_size):\n",
    "    X_train, y_train = train_dataset\n",
    "    return (X_train[:train_size,...], y_train[:train_size,...]), (X_train[train_size:,...], y_train[train_size:,...])\n",
    "\n",
    "def load_cifar_train_val():\n",
    "    train_dataset, _ = cifar100.load_data()\n",
    "    \n",
    "    train_dataset, validation_dataset = train_val_split(train_dataset, validation_size)\n",
    "    \n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices(validation_dataset).map(preprocessing).batch(batch_size).map(preprocess_resnet).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset).map(preprocessing).cache().repeat().shuffle(train_size).batch(batch_size).map(augmentation_donnees, num_parallel_calls = tf.data.AUTOTUNE).map(preprocess_resnet, num_parallel_calls = tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73061cae-6b54-425c-8a4c-3efb3fe929cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modele_logits(modele):\n",
    "    config = modele.layers[-1].get_config()\n",
    "    config['activation'] = linear\n",
    "    config['name'] = 'logits'\n",
    "    res = Model(inputs=modele.inputs, outputs=[Dense(**config)(modele.layers[-2].output)])\n",
    "    res.layers[-1].set_weights([x.numpy() for x in modele.layers[-1].weights])\n",
    "    res.compile(metrics=['accuracy'])\n",
    "    return res\n",
    "\n",
    "@tf.function\n",
    "def compte_bons(x,y):\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(tf.argmax(x, axis = 1), tf.argmax(y, axis = 1)), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def softmax(logits, temp):\n",
    "    expo = tf.exp(logits / temp)\n",
    "    return expo / tf.reduce_sum(expo, axis = 1, keepdims=True)\n",
    "\n",
    "@tf.function\n",
    "def ce(x, y_logits, temp):\n",
    "    return softmax_cross_entropy_with_logits(x, y_logits / temp) * temp**2\n",
    "\n",
    "def init_csv_log(fichier):\n",
    "    with open(fichier,'w') as file:\n",
    "        file.write(\"epoch, accuracy_etu,val_accuracy_etu,val_accuracy_ens,val_accuracy_dis\\n\")\n",
    "def append_csv_log(fichier, epoch, accuracy_etu, val_accuracy_etu, val_accuracy_ens, val_accuracy_dis):\n",
    "    with open(fichier,'a') as file:\n",
    "        file.write(f\"{epoch:d},{accuracy_etu:.2f},{val_accuracy_etu:.2f},{val_accuracy_ens:.2f},{val_accuracy_dis:.2f}\\n\")\n",
    "\n",
    "def forward_backward_pass_impl(train_dataset_iter, enseignant_logit_model, etudiant_logit_model, temp, optim_enseignant, optim_etudiant):\n",
    "    X_batch, y_batch = next(train_dataset_iter)\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        etudiant_estim_logit = etudiant_logit_model(X_batch, training = True)\n",
    "        enseignant_estim_logit = enseignant_logit_model(X_batch, training = True)\n",
    "        enseignant_estim_softmax = softmax(enseignant_estim_logit, temp)\n",
    "        perte = softmax_cross_entropy_with_logits(y_batch,etudiant_estim_logit) + softmax_cross_entropy_with_logits(y_batch,enseignant_estim_logit) + ce(enseignant_estim_softmax,etudiant_estim_logit, temp)\n",
    "    grad_etudiant = tape.gradient(perte, etudiant_logit_model.trainable_variables)\n",
    "    grad_enseignant = tape.gradient(perte, enseignant_logit_model.trainable_variables)\n",
    "    optim_etudiant.apply_gradients(zip(grad_etudiant, etudiant_logit_model.trainable_variables))\n",
    "    optim_enseignant.apply_gradients(zip(grad_enseignant, enseignant_logit_model.trainable_variables))\n",
    "    return compte_bons(etudiant_estim_logit,y_batch), compte_bons(enseignant_estim_logit,y_batch), compte_bons(etudiant_estim_logit,enseignant_estim_logit)\n",
    "\n",
    "@tf.function\n",
    "def val_accuracies(etudiant, enseignant, validation_dataset):\n",
    "    val_bons_etu, val_bons_ens, val_bons_dis = 0, 0, 0\n",
    "    for X_batch, y_batch in validation_dataset:\n",
    "        etu_prev = etudiant(X_batch, training = False)\n",
    "        ens_prev = enseignant(X_batch, training = False)\n",
    "        val_accuracies += compte_bons(etu_prev, y_batch)\n",
    "        val_bons_ens += compte_bons(ens_prev, y_batch)\n",
    "        val_bons_dis += compte_bons(etu_prev, ens_prev)\n",
    "    nb_obs_val = cast((validation_size//batch_size) * batch_size, tf.float32)\n",
    "    val_accuracy_etu = cast(val_bons_etu, tf.float32) /  nb_obs_val\n",
    "    val_accuracy_ens = cast(val_bons_ens, tf.float32) /  nb_obs_val\n",
    "    val_accuracy_dis = cast(val_bons_dis, tf.float32) /  nb_obs_val\n",
    "    return val_accuracy_etu, val_accuracy_ens, val_accuracy_dis\n",
    "\n",
    "def distillateur_kl_en_ligne(etudiant, enseignant, train_dataset, validation_dataset, temp, nom_modele, n_epoch):\n",
    "    etudiant_logit_model = get_modele_logits(etudiant)\n",
    "    enseignant_logit_model = get_modele_logits(enseignant)\n",
    "    optim_etudiant = SGD(learning_rate=0.001)\n",
    "    optim_enseignant = Adam(learning_rate=0.001)\n",
    "    init_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\")\n",
    "    print(\"C'est parti pour la distillation !\\n\")\n",
    "    val_accuracy_etu_max, val_accuracy_ens_max, val_accuracy_dis_max  = 0, 0, 0\n",
    "    forward_backward_pass = tf.function(forward_backward_pass_impl)\n",
    "    train_dataset_iter = iter(train_dataset)\n",
    "    # La tf.function ne peut être que locale car son graphe dépend d'étudiant_logit_model et sinon Tensorflow renvoie une erreur à deux applications successives\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Époque {epoch + 1} / {n_epoch}\")\n",
    "        n_batch = train_size//batch_size\n",
    "        barre_progression = Progbar(n_batch, stateful_metrics = [\"acc\"])\n",
    "        bons_epoque_etu, bons_epoque_ens, bons_epoque_dis = 0, 0, 0\n",
    "        for i in range(n_batch):\n",
    "            bons_etu, bons_ens, bons_dis = forward_backward_pass(train_dataset_iter, enseignant_logit_model, etudiant_logit_model, temp, optim_etudiant, optim_enseignant)\n",
    "            bons_epoque_etu += bons_etu.numpy()\n",
    "            bons_epoque_ens += bons_ens.numpy()\n",
    "            bons_epoque_dis += bons_dis.numpy()\n",
    "            n_observ = (i+1) * batch_size\n",
    "            accuracy_etu, accuracy_ens, accuracy_dis = bons_epoque_etu / n_observ, bons_epoque_ens / n_observ, bons_epoque_dis / n_observ\n",
    "            barre_progression.update(i + 1, values = [(\"acc (etu, train) : \", accuracy_etu), (\"acc (ens, train) : \", accuracy_ens), (\"acc (dis, train) : \", accuracy_dis)])\n",
    "        val_accuracy_etu, val_accuracy_ens, val_accuracy_dis = val_accuracies(etudiant, enseignant, validation_dataset)\n",
    "        if val_accuracy_etu > val_accuracy_etu_max:\n",
    "            val_accuracy_etu_max = val_accuracy_etu\n",
    "            etudiant.save(f\"sauvegardes/{nom_modele}_checkpoint.keras\")\n",
    "        if val_accuracy_ens > val_accuracy_ens_max:\n",
    "            val_accuracy_ens_max = val_accuracy_ens\n",
    "        if val_accuracy_dis > val_accuracy_dis_max:\n",
    "            val_accuracy_dis_max = val_accuracy_dis\n",
    "        if epoch + 1 == 50:\n",
    "            optim_etudiant.learning_rate.assign(0.0001)\n",
    "            optim_enseignant.learning_rate.assign(0.0001)\n",
    "        if epoch + 1 in [10,20,50]:\n",
    "            print(f\"---> Epoque {epoch + 1:d} - Max val accuracy -> etu : {val_accuracy_etu_max:.4f} | ens : {val_accuracy_ens_max:.4f} | dis : {val_accuracy_dis_max:.4f}\")\n",
    "        append_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\", epoch, accuracy_etu, val_accuracy_etu, val_accuracy_ens, val_accuracy_dis)\n",
    "        print(f\"Accuracy (etu, val) : {val_accuracy_etu:.4f} | Accuracy (ens, val) : {val_accuracy_ens:.4f} | Accuracy (dis, val) : {val_accuracy_dis:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7370e7-7f99-43ca-8de5-42deaf10cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_resnet18_en_ligne(temp):\n",
    "    clear_session()\n",
    "    train_dataset, validation_dataset = load_cifar_train_val()\n",
    "    modele = new_modele_resnet()\n",
    "    nom_modele =  f\"model_etudiant_ligne_t{temp:d}\"\n",
    "    distillateur_kl_en_ligne(modele, model_enseignant, train_dataset, validation_dataset, temp, nom_modele, n_epoch)\n",
    "    wd = os.getcwd()\n",
    "    os.system(f\"cp {wd}/sauvegardes/{nom_modele}_checkpoint.keras {wd}/sauvegardes/{nom_modele}.keras\")\n",
    "    os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}.keras s3/afeldmann/projet_cnam/{nom_modele}.keras\")\n",
    "    os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}_logs.csv s3/afeldmann/projet_cnam/{nom_modele}_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0906365-7153-44d7-b06c-277a8de12fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    resnet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    resnet = resnet18((224, 224, 3), weights='imagenet', include_top=False)\n",
    "    resnet_output = GlobalAveragePooling2D()(resnet.output)\n",
    "    resnet = Model(inputs=resnet.input, outputs=resnet_output)\n",
    "    return resnet\n",
    "\n",
    "def new_modele_resnet():\n",
    "    model = Sequential([\n",
    "        Input((224,224,3)),\n",
    "        ResNet18(),\n",
    "        Dropout(0.25),\n",
    "        Dense(256, activation=\"sigmoid\", kernel_regularizer = tf.keras.regularizers.L1(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\", kernel_regularizer = tf.keras.regularizers.L2(0.001))\n",
    "    ])\n",
    "    model.compile(metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b567ca39-ce1f-43cf-9fe4-f9ccb4a4ad9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est parti pour la distillation !\n",
      "\n",
      "Époque 1 / 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 12:50:27.679183: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1045s\u001b[0m 1s/step - acc (etu, train) : : 0.0610 - acc (ens, train) : : 0.0978 - acc (dis, train) : : 0.0750\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_41124/3110802199.py\", line 49, in val_accuracies  *\n        val_accuracy_etu += tf.cast(compte_bons(etu_prev, y_batch), tf.float32)\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistillation_resnet18_en_ligne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mdistillation_resnet18_en_ligne\u001b[0;34m(temp)\u001b[0m\n\u001b[1;32m      4\u001b[0m modele \u001b[38;5;241m=\u001b[39m new_modele_resnet()\n\u001b[1;32m      5\u001b[0m nom_modele \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_etudiant_ligne_t\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdistillateur_kl_en_ligne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodele\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_enseignant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnom_modele\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m wd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sauvegardes/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnom_modele\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_checkpoint.keras \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sauvegardes/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnom_modele\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 82\u001b[0m, in \u001b[0;36mdistillateur_kl_en_ligne\u001b[0;34m(etudiant, enseignant, train_dataset, validation_dataset, temp, nom_modele, n_epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m     accuracy_etu, accuracy_ens, accuracy_dis \u001b[38;5;241m=\u001b[39m bons_epoque_etu \u001b[38;5;241m/\u001b[39m n_observ, bons_epoque_ens \u001b[38;5;241m/\u001b[39m n_observ, bons_epoque_dis \u001b[38;5;241m/\u001b[39m n_observ\n\u001b[1;32m     81\u001b[0m     barre_progression\u001b[38;5;241m.\u001b[39mupdate(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, values \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc (etu, train) : \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_etu), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc (ens, train) : \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_ens), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc (dis, train) : \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_dis)])\n\u001b[0;32m---> 82\u001b[0m val_accuracy_etu, val_accuracy_ens, val_accuracy_dis \u001b[38;5;241m=\u001b[39m \u001b[43mval_accuracies\u001b[49m\u001b[43m(\u001b[49m\u001b[43metudiant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menseignant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_accuracy_etu \u001b[38;5;241m>\u001b[39m val_accuracy_etu_max:\n\u001b[1;32m     84\u001b[0m     val_accuracy_etu_max \u001b[38;5;241m=\u001b[39m val_accuracy_etu\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileh92m5x63.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__val_accuracies\u001b[0;34m(etudiant, enseignant, validation_dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m ens_prev \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mens_prev\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_batch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy_dis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy_ens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy_etu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(X_batch, y_batch)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m nb_obs_val \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(validation_size) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(batch_size) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(batch_size)\n\u001b[1;32m     36\u001b[0m val_accuracy_etu \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(val_accuracy_etu)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileh92m5x63.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__val_accuracies.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     23\u001b[0m ens_prev \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(enseignant), (ag__\u001b[38;5;241m.\u001b[39mld(X_batch),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)\n\u001b[1;32m     24\u001b[0m val_accuracy_etu \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(val_accuracy_etu)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mval_accuracy_etu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompte_bons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43metu_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m val_accuracy_ens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(val_accuracy_ens)\n\u001b[1;32m     27\u001b[0m val_accuracy_ens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(tf\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(compte_bons, (ens_prev, y_batch), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), tf\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_41124/3110802199.py\", line 49, in val_accuracies  *\n        val_accuracy_etu += tf.cast(compte_bons(etu_prev, y_batch), tf.float32)\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "distillation_resnet18_en_ligne(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98bde8-245e-4aae-abe4-777810904e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
