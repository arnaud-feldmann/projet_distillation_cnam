{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kc8YkE46d5Sk",
    "outputId": "cfd3c208-2b49-47aa-a433-f2ea43363125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_model_optimization in /opt/mamba/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: tf_keras in /opt/mamba/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: image_classifiers in /opt/mamba/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /opt/mamba/lib/python3.11/site-packages (from tensorflow_model_optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow_model_optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in /opt/mamba/lib/python3.11/site-packages (from tensorflow_model_optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /opt/mamba/lib/python3.11/site-packages (from tensorflow_model_optimization) (1.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/mamba/lib/python3.11/site-packages (from tf_keras) (2.16.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/mamba/lib/python3.11/site-packages (from image_classifiers) (1.0.8)\n",
      "Requirement already satisfied: h5py in /opt/mamba/lib/python3.11/site-packages (from keras-applications<=1.0.8,>=1.0.7->image_classifiers) (3.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (69.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/mamba/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.36.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/mamba/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/mamba/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/mamba/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/mamba/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/mamba/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/mamba/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_model_optimization tf_keras image_classifiers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "from tf_keras.applications import ResNet50V2\n",
    "from tf_keras.datasets import cifar100\n",
    "from tf_keras import Sequential, Input\n",
    "from tf_keras.layers import Dense, Dropout, RandomFlip, RandomTranslation, RandomRotation,RandomBrightness, RandomContrast, RandomZoom, GlobalAveragePooling2D\n",
    "from tf_keras.applications.resnet_v2 import preprocess_input\n",
    "from tf_keras.losses import CategoricalCrossentropy\n",
    "from tf_keras.activations import linear\n",
    "from tf_keras.models import Model\n",
    "from tf_keras.backend import clear_session\n",
    "from tf_keras.optimizers import SGD\n",
    "from tf_keras.utils import Progbar\n",
    "from tensorflow.nn import softmax_cross_entropy_with_logits\n",
    "from classification_models.models_factory import ModelsFactory\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras.pruning_wrapper import PruneLowMagnitude\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelsFactory(ModelsFactory):\n",
    "    @staticmethod\n",
    "    def get_kwargs():\n",
    "        return {\n",
    "            'backend': keras.backend,\n",
    "            'layers': keras.layers,\n",
    "            'models': keras.models,\n",
    "            'utils': keras.utils,\n",
    "        }\n",
    "# Pour faire marcher image-classifiers avec tf_keras, la version compatibilité de tf.keras, nécéssaire pour tflite\n",
    "Classifiers = KerasModelsFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 40\n",
    "batch_size = 100\n",
    "num_classes = 100\n",
    "n_images = 50000 # Pour l'entrainement, et 10000 pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uPDP8KyWfCat"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...nant.keras: 135.73 MiB / 135.73 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 50.26 MiB/s 2s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc cp s3/afeldmann/projet_cnam/modele_enseignant.keras /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\n",
    "model_enseignant = Sequential([\n",
    "    Input((224,224,3)),\n",
    "    ResNet50V2(include_top=False, weights='imagenet', pooling=\"avg\"),\n",
    "    Dropout(0.25),\n",
    "    Dense(256, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.L1(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\", kernel_regularizer = keras.regularizers.L2(0.001))\n",
    "])\n",
    "# Keras 3.1.1 est buggé et le chargement direct ne marche pas ici, même si les poids sont bien enregistrés\n",
    "model_enseignant.load_weights(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\")\n",
    "\n",
    "model_enseignant.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    resnet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    resnet = resnet18((224, 224, 3), weights='imagenet', include_top=False)\n",
    "    resnet_output = GlobalAveragePooling2D()(resnet.output)\n",
    "    resnet = Model(inputs=resnet.input, outputs=resnet_output)\n",
    "    return resnet\n",
    "\n",
    "def new_modele_resnet():\n",
    "    model = Sequential([\n",
    "        Input((224,224,3)),\n",
    "        ResNet18(),\n",
    "        Dropout(0.25),\n",
    "        Dense(256, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.L1(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\", kernel_regularizer = keras.regularizers.L2(0.001))\n",
    "    ])\n",
    "    model.compile(metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..._a50.keras: 43.54 MiB / 43.54 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 96.66 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'RMSprop', because it has 1 variables whereas the saved optimizer has 2 variables. \n"
     ]
    }
   ],
   "source": [
    "!mc cp s3/afeldmann/projet_cnam/model_etudiant_t3_a50.keras /home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\n",
    "model_etudiant = new_modele_resnet()\n",
    "model_etudiant.load_weights(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\")\n",
    "model_etudiant.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bd51EjqzvhP7"
   },
   "outputs": [],
   "source": [
    "def preprocessing(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    label = tf.squeeze(tf.one_hot(label, depth = num_classes), axis = 0)\n",
    "    return  image, label\n",
    "\n",
    "augmentation_donnees_keras = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.2,0.2),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "    RandomContrast(0.2),\n",
    "    RandomBrightness(0.2,value_range=(0,1))\n",
    "])\n",
    "\n",
    "def augmentation_donnees(image, label):\n",
    "    return augmentation_donnees_keras(image/255.0, training = True)*255.0, label\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    return preprocess_input(image), label\n",
    "\n",
    "def load_cifar_train():\n",
    "    train_dataset, _ = cifar100.load_data()\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset).map(preprocessing).cache().repeat().shuffle(n_images).batch(batch_size).map(augmentation_donnees, num_parallel_calls = tf.data.AUTOTUNE).map(preprocess_resnet, num_parallel_calls = tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_dataset\n",
    "\n",
    "_, test_dataset = cifar100.load_data()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_dataset).map(preprocessing).batch(batch_size).map(preprocess_resnet, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modele_logits(modele):\n",
    "    config = modele.layers[-1].get_config()\n",
    "    if isinstance(modele.layers[-1],PruneLowMagnitude):\n",
    "        config['layer']['config']['name'] = 'logits'\n",
    "        config['layer']['config']['activation'] = linear\n",
    "    else:\n",
    "        config['name'] = 'logits'\n",
    "        config['activation'] = linear\n",
    "    res = Model(inputs=modele.inputs, outputs=[type(modele.layers[-1]).from_config(config)(modele.layers[-2].output)])\n",
    "    res.layers[-1].set_weights(modele.layers[-1].get_weights())\n",
    "    res.compile(metrics=['accuracy'])\n",
    "    return res\n",
    "\n",
    "def get_regularisation_pruning(model):\n",
    "    return [(getattr(layer.layer,reg),layer.layer.kernel) for layer in model.layers if hasattr(layer, \"layer\") for reg in [\"kernel_regularizer\", \"bias_regularizer\"] if hasattr(layer.layer, reg) and getattr(layer.layer,reg) is not None]\n",
    "\n",
    "@tf.function\n",
    "def perte_regularisation(regularisation):\n",
    "    return tf.add_n([tf.reduce_sum(reg(kernel)) for reg, kernel in regularisation])\n",
    "\n",
    "@tf.function\n",
    "def compte_bons(x,y):\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(tf.argmax(x, axis = 1), tf.argmax(y, axis = 1)), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def softmax(logits, temp):\n",
    "    expo = tf.exp(logits / temp)\n",
    "    return expo / tf.reduce_sum(expo, axis = 1, keepdims=True)\n",
    "\n",
    "@tf.function\n",
    "def ce(x, y_logits, temp):\n",
    "    return softmax_cross_entropy_with_logits(x, y_logits / temp) * temp**2\n",
    "\n",
    "def init_csv_log(fichier):\n",
    "    with open(fichier,'w') as file:\n",
    "        file.write(\"epoch, accuracy_etu, accuracy_dis\\n\")\n",
    "def append_csv_log(fichier, epoch, accuracy_etu, accuracy_dis):\n",
    "    with open(fichier,'a') as file:\n",
    "        file.write(f\"{epoch:d},{accuracy_etu:.2f},{accuracy_dis:.2f}\\n\")\n",
    "\n",
    "def forward_backward_pass_impl(train_dataset_iter, etudiant_logit_model, alpha, temp, optim, regularisation_etudiant):\n",
    "    X_batch, y_batch, enseignant_estim_softmax = next(train_dataset_iter)\n",
    "    with tf.GradientTape() as tape:\n",
    "        etudiant_estim_logit = etudiant_logit_model(X_batch, training = True)\n",
    "        perte = alpha * softmax_cross_entropy_with_logits(y_batch,etudiant_estim_logit) + (1-alpha) * ce(enseignant_estim_softmax,etudiant_estim_logit, temp) + perte_regularisation(regularisation_etudiant)\n",
    "    grads = tape.gradient(perte, etudiant_logit_model.trainable_variables)\n",
    "    optim.apply_gradients(zip(grads, etudiant_logit_model.trainable_variables))\n",
    "    return compte_bons(etudiant_estim_logit,y_batch), compte_bons(etudiant_estim_logit, enseignant_estim_softmax)\n",
    "\n",
    "def distillateur_kl_pruning(etudiant_pruning, enseignant, train_dataset, temp, nom_modele, n_epoch, alpha):\n",
    "    etudiant_logit_model = get_modele_logits(etudiant_pruning)\n",
    "    enseignant_logit_model = get_modele_logits(enseignant)\n",
    "    optim = SGD(learning_rate=0.001)\n",
    "    init_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\")\n",
    "    print(\"C'est parti pour la distillation !\\n\")\n",
    "    train_dataset_iter = iter(\n",
    "        train_dataset\n",
    "        .map(lambda images, label: (images, label, softmax(enseignant_logit_model(images, training = False), temp)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    regularisation_etudiant = get_regularisation_pruning(etudiant_pruning)\n",
    "    forward_backward_pass = tf.function(forward_backward_pass_impl)\n",
    "    step_callback = tfmot.sparsity.keras.UpdatePruningStep()\n",
    "    step_callback.set_model(etudiant_logit_model)\n",
    "    step_callback.on_train_begin()\n",
    "    # La tf.function ne peut être que locale car son graphe dépend d'étudiant_logit_model et sinon Tensorflow renvoie une erreur à deux applications successives\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Époque {epoch + 1} / {n_epoch}\")\n",
    "        n_batch = n_images//batch_size\n",
    "        barre_progression = Progbar(n_batch, stateful_metrics = [\"acc (etu, train)\", \"acc (dis, train)\"])\n",
    "        bons_epoque_etu, bons_epoque_dis = 0, 0\n",
    "        for i in range(n_batch):\n",
    "            if i == 10:\n",
    "                optim.learning_rate.assign(0.0005)\n",
    "            if i == 19:\n",
    "                optim.learning_rate.assign(0.00025)\n",
    "            if i == 29:\n",
    "                optim.learning_rate.assign(0.0001)\n",
    "            step_callback.on_train_batch_begin(batch=-1)\n",
    "            bons_etu, bons_dis = forward_backward_pass(train_dataset_iter, etudiant_logit_model, alpha, temp, optim, regularisation_etudiant)\n",
    "            bons_epoque_etu += bons_etu.numpy()\n",
    "            bons_epoque_dis += bons_dis.numpy()\n",
    "            n_observ = (i+1) * batch_size\n",
    "            accuracy_etu, accuracy_dis = bons_epoque_etu / n_observ, bons_epoque_dis / n_observ\n",
    "            barre_progression.update(i + 1, values = [(\"acc (etu, train)\", accuracy_etu), (\"acc (dis, train)\", accuracy_dis)])\n",
    "        append_csv_log(f\"sauvegardes/{nom_modele}_logs.csv\", epoch, accuracy_etu, accuracy_dis)\n",
    "        step_callback.on_epoch_end(batch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_step = np.ceil(n_images / batch_size).astype(np.int32) * n_epoch\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.20,\n",
    "                                                               final_sparsity=0.65,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_etudiant_pruning = tfmot.sparsity.keras.prune_low_magnitude(model_etudiant, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est parti pour la distillation !\n",
      "\n",
      "Époque 1 / 40\n",
      "500/500 [==============================] - 1917s 4s/step - acc (etu, train): 0.7338 - acc (dis, train): 0.7237\n",
      "Époque 2 / 40\n",
      "500/500 [==============================] - 1809s 4s/step - acc (etu, train): 0.7550 - acc (dis, train): 0.7474\n",
      "Époque 3 / 40\n",
      "500/500 [==============================] - 1786s 4s/step - acc (etu, train): 0.7643 - acc (dis, train): 0.7553\n",
      "Époque 4 / 40\n",
      "500/500 [==============================] - 1776s 4s/step - acc (etu, train): 0.7698 - acc (dis, train): 0.7601\n",
      "Époque 5 / 40\n",
      "500/500 [==============================] - 1783s 4s/step - acc (etu, train): 0.7736 - acc (dis, train): 0.7679\n",
      "Époque 6 / 40\n",
      "500/500 [==============================] - 1985s 4s/step - acc (etu, train): 0.7721 - acc (dis, train): 0.7641\n",
      "Époque 7 / 40\n",
      "500/500 [==============================] - 1984s 4s/step - acc (etu, train): 0.7705 - acc (dis, train): 0.7626\n",
      "Époque 8 / 40\n",
      "500/500 [==============================] - 1961s 4s/step - acc (etu, train): 0.7693 - acc (dis, train): 0.7634\n",
      "Époque 9 / 40\n",
      "500/500 [==============================] - 1845s 4s/step - acc (etu, train): 0.7653 - acc (dis, train): 0.7572\n",
      "Époque 10 / 40\n",
      "500/500 [==============================] - 1741s 3s/step - acc (etu, train): 0.7628 - acc (dis, train): 0.7546\n",
      "Époque 11 / 40\n",
      "500/500 [==============================] - 1736s 3s/step - acc (etu, train): 0.7566 - acc (dis, train): 0.7497\n",
      "Époque 12 / 40\n",
      "500/500 [==============================] - 1726s 3s/step - acc (etu, train): 0.7517 - acc (dis, train): 0.7462\n",
      "Époque 13 / 40\n",
      "500/500 [==============================] - 1729s 3s/step - acc (etu, train): 0.7467 - acc (dis, train): 0.7400\n",
      "Époque 14 / 40\n",
      "500/500 [==============================] - 1764s 4s/step - acc (etu, train): 0.7434 - acc (dis, train): 0.7368\n",
      "Époque 15 / 40\n",
      "500/500 [==============================] - 1846s 4s/step - acc (etu, train): 0.7353 - acc (dis, train): 0.7305\n",
      "Époque 16 / 40\n",
      "500/500 [==============================] - 1728s 3s/step - acc (etu, train): 0.7291 - acc (dis, train): 0.7238\n",
      "Époque 17 / 40\n",
      "500/500 [==============================] - 1724s 3s/step - acc (etu, train): 0.7263 - acc (dis, train): 0.7205\n",
      "Époque 18 / 40\n",
      "500/500 [==============================] - 1719s 3s/step - acc (etu, train): 0.7207 - acc (dis, train): 0.7148\n",
      "Époque 19 / 40\n",
      "500/500 [==============================] - 1724s 3s/step - acc (etu, train): 0.7186 - acc (dis, train): 0.7136\n",
      "Époque 20 / 40\n",
      "500/500 [==============================] - 1721s 3s/step - acc (etu, train): 0.7105 - acc (dis, train): 0.7060\n",
      "Époque 21 / 40\n",
      "500/500 [==============================] - 1721s 3s/step - acc (etu, train): 0.7075 - acc (dis, train): 0.7041\n",
      "Époque 22 / 40\n",
      "500/500 [==============================] - 1761s 4s/step - acc (etu, train): 0.7111 - acc (dis, train): 0.7052\n",
      "Époque 23 / 40\n",
      "500/500 [==============================] - 1780s 4s/step - acc (etu, train): 0.7056 - acc (dis, train): 0.7017\n",
      "Époque 24 / 40\n",
      "500/500 [==============================] - 1943s 4s/step - acc (etu, train): 0.7050 - acc (dis, train): 0.6991\n",
      "Époque 25 / 40\n",
      "500/500 [==============================] - 1729s 3s/step - acc (etu, train): 0.7061 - acc (dis, train): 0.7015\n",
      "Époque 26 / 40\n",
      "500/500 [==============================] - 1718s 3s/step - acc (etu, train): 0.7056 - acc (dis, train): 0.7006\n",
      "Époque 27 / 40\n",
      "500/500 [==============================] - 1726s 3s/step - acc (etu, train): 0.7056 - acc (dis, train): 0.7035\n",
      "Époque 28 / 40\n",
      "500/500 [==============================] - 1825s 4s/step - acc (etu, train): 0.7055 - acc (dis, train): 0.7021\n",
      "Époque 29 / 40\n",
      "500/500 [==============================] - 1786s 4s/step - acc (etu, train): 0.7087 - acc (dis, train): 0.7029\n",
      "Époque 30 / 40\n",
      "500/500 [==============================] - 1768s 4s/step - acc (etu, train): 0.7048 - acc (dis, train): 0.7007\n",
      "Époque 31 / 40\n",
      "500/500 [==============================] - 1721s 3s/step - acc (etu, train): 0.7045 - acc (dis, train): 0.7019\n",
      "Époque 32 / 40\n",
      "500/500 [==============================] - 1713s 3s/step - acc (etu, train): 0.7095 - acc (dis, train): 0.7052\n",
      "Époque 33 / 40\n",
      "500/500 [==============================] - 1712s 3s/step - acc (etu, train): 0.7113 - acc (dis, train): 0.7061\n",
      "Époque 34 / 40\n",
      "500/500 [==============================] - 1707s 3s/step - acc (etu, train): 0.7112 - acc (dis, train): 0.7075\n",
      "Époque 35 / 40\n",
      "500/500 [==============================] - 1699s 3s/step - acc (etu, train): 0.7139 - acc (dis, train): 0.7109\n",
      "Époque 36 / 40\n",
      "500/500 [==============================] - 1715s 3s/step - acc (etu, train): 0.7171 - acc (dis, train): 0.7130\n",
      "Époque 37 / 40\n",
      "500/500 [==============================] - 1714s 3s/step - acc (etu, train): 0.7185 - acc (dis, train): 0.7132\n",
      "Époque 38 / 40\n",
      "500/500 [==============================] - 1726s 3s/step - acc (etu, train): 0.7146 - acc (dis, train): 0.7112\n",
      "Époque 39 / 40\n",
      "500/500 [==============================] - 1831s 4s/step - acc (etu, train): 0.7193 - acc (dis, train): 0.7145\n",
      "Époque 40 / 40\n",
      "500/500 [==============================] - 1833s 4s/step - acc (etu, train): 0.7178 - acc (dis, train): 0.7118\n",
      "`/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant_pruning.keras` -> `s3/afeldmann/projet_cnam/model_etudiant_pruning.keras`\n",
      "Total: 130.22 MiB, Transferred: 130.22 MiB, Speed: 251.83 MiB/s\n",
      "`/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant_pruning_logs.csv` -> `s3/afeldmann/projet_cnam/model_etudiant_pruning_logs.csv`\n",
      "Total: 544 B, Transferred: 544 B, Speed: 2.33 KiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_cifar_train()\n",
    "nom_modele =  f\"model_etudiant_pruning\"\n",
    "distillateur_kl_pruning(model_etudiant_pruning, model_enseignant, train_dataset, 3, nom_modele, n_epoch, 0.5)\n",
    "wd = os.getcwd()\n",
    "model_etudiant_pruning.save(f\"{wd}/sauvegardes/{nom_modele}.keras\")\n",
    "os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}.keras s3/afeldmann/projet_cnam/{nom_modele}.keras\")\n",
    "os.system(f\"mc cp {wd}/sauvegardes/{nom_modele}_logs.csv s3/afeldmann/projet_cnam/{nom_modele}_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 71s 562ms/step - loss: 0.9232 - accuracy: 0.6541\n",
      "100/100 [==============================] - 130s 1s/step - loss: 0.9232 - accuracy: 0.6555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9232019782066345, 0.6554999947547913]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_etudiant_pruning.compile(metrics=\"accuracy\")\n",
    "model_etudiant_pruning.evaluate(test_dataset)\n",
    "model_etudiant_pruning.evaluate(test_dataset.map(lambda images, labels: (images,model_enseignant(images, training = False)), num_parallel_calls = tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nkO80TA8Di2",
    "outputId": "30f808df-b05d-4b70-87db-66fbc0a80dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16701/3508845441.py:3: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_etudiant_pruning)\n",
    "pruned_keras_file = \"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.h5\"\n",
    "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS6JihkL9dcL",
    "outputId": "664d79e8-36bf-42b5-d68b-88e6c1441de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6za4i7vc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6za4i7vc/assets\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1714625729.492644   16701 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1714625729.492742   16701 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-02 04:55:29.503233: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp6za4i7vc\n",
      "2024-05-02 04:55:29.527130: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-02 04:55:29.527167: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp6za4i7vc\n",
      "2024-05-02 04:55:29.606836: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-05-02 04:55:29.618061: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-02 04:55:29.819822: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmp6za4i7vc\n",
      "2024-05-02 04:55:29.883249: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 380026 microseconds.\n",
      "2024-05-02 04:55:30.394524: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pruned_tflite_model = converter.convert()\n",
    "pruned_tflite_file = \"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.tflite\"\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-lFfpMf_ncg",
    "outputId": "c8095dae-df05-4214-a4fb-b9b05bed72d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [01:41, 17.34it/s]2024-05-02 05:00:46.893744: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "1000it [01:44,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653 prédictions correctes sur 1000 face aux étiquettes dures et 644 face aux étiquettes douces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_batches = test_dataset.map(lambda images, labels: (images,labels,model_enseignant(images, training = False)), num_parallel_calls = tf.data.AUTOTUNE).unbatch().batch(1)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=pruned_tflite_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_etupreds, test_labels, test_enspreds = [], [], []\n",
    "for img, label, enspred in tqdm(test_batches.take(1000)):\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    test_etupreds.append(interpreter.get_tensor(output_index))\n",
    "    test_labels.append(label.numpy()[0])\n",
    "    test_enspreds.append(enspred.numpy()[0])\n",
    "\n",
    "score_dur, score_dis = 0, 0\n",
    "for item in range(0,len(test_etupreds)):\n",
    "    etupred = np.argmax(test_etupreds[item])\n",
    "    label = np.argmax(test_labels[item])\n",
    "    enspred = np.argmax(test_enspreds[item])\n",
    "    if etupred == label:\n",
    "        score_dur += 1\n",
    "    if etupred == enspred:\n",
    "        score_dis += 1\n",
    "\n",
    "print(f\"{score_dur} prédictions correctes sur 1000 face aux étiquettes dures et {score_dis} face aux étiquettes douces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du modèle enseignant zippé : 131559019.00 bytes\n",
      "Taille du modèle étudiant zippé : 42143482.00 bytes\n",
      "Taille du modèle étudiant zippé après élagage : 19847419.00 bytes\n",
      "Taille du modèle étudiant zippé après élagage et quantification : 5684626.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille du modèle enseignant zippé : %.2f bytes\" % (get_gzipped_model_size(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_enseignant.keras\")))\n",
    "print(\"Taille du modèle étudiant zippé : %.2f bytes\" % (get_gzipped_model_size(\"/home/onyxia/work/projet_distillation_cnam/sauvegardes/model_etudiant.keras\")))\n",
    "print(\"Taille du modèle étudiant zippé après élagage : %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Taille du modèle étudiant zippé après élagage et quantification : %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...ing.tflite: 10.91 MiB / 10.91 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 14.18 MiB/s 0s\u001b[0;22m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b]11;?\u001b\\\u001b[6n"
     ]
    }
   ],
   "source": [
    "!mc cp /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.tflite s3/afeldmann/projet_cnam/modele_etudiant_pruning.tflite\n",
    "!mc cp /home/onyxia/work/projet_distillation_cnam/sauvegardes/modele_etudiant_pruning.h5 s3/afeldmann/projet_cnam/modele_etudiant_pruning.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKJ8xfxqA35dOVM+IFKB3Q",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1zfo5zNxKu097eBmdA-BrPQHD7kJFJxRZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
